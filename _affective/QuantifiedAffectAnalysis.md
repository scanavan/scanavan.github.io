---
title: "Quantified Facial Affect Analysis"
collection: affective
---

# Abstract
The quantification of visual affect data (e.g. face images) is essential to build and monitor automated affect modeling systems efficiently. Considering this, this work proposes quantified facial Temporal-expressiveness Dynamics (TED) to quantify the expressiveness of human faces. The proposed algorithm leverages multimodal facial features by incorporating static and dynamic information to enable accurate measurements of facial expressiveness. We show that TED can be used for high-level tasks such as summarization of unstructured visual data, and expectation from and interpretation of automated affect recognition models. To evaluate the positive impact of using TED, a case study was conducted on spontaneous pain using the UNBC-McMaster spontaneous shoulder pain dataset. Experimental results show the efficacy of using TED for quantified affect analysis.

# Papers
1. [Md T. Uddin and S. Canavan. Quantified Facial Temporal-Expressiveness Dynamics for Affect Analysis, ICPR, 2020.](/files/TED_ICPR_2020.pdf) 
2. [Md T. Uddin and S. Canavan. Quantified Facial Expressiveness for Affective Behavior Analytics, WACV, 2022.](/files/WACV2022.pdf)